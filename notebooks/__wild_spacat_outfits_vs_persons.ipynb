{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image, ImageShow\n",
    "import PIL\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from diffusers.utils import make_image_grid\n",
    "from IPython.core.debugger import Pdb\n",
    "from einops import rearrange\n",
    "import cv2\n",
    "from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n",
    "import numpy as np\n",
    "import pprint\n",
    "import runpy\n",
    "import sys\n",
    "from src.models.attention_processor import SkipAttnProcessor\n",
    "from src.pipelines.spacat_pipeline import TryOnPipeline\n",
    "from src.dataset.vitonhd import VITONHDDataset\n",
    "from src.dataset.dresscode import DressCodeDataset\n",
    "from src.utils import get_project_root, show, init_attn_processor\n",
    "from src.preprocess.humanparsing.run_parsing import Parsing\n",
    "from src.preprocess.openpose.run_openpose import OpenPose\n",
    "from src.utils.mask import get_mask_location\n",
    "from src.preprocess import apply_net\n",
    "from src.utils import get_project_root, mask2agn\n",
    "from src.utils.mask_v2 import Maskerv2 as Masker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h, w = (512, 384)\n",
    "# h, w = (1024, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge(\n",
    "    img1: PIL.Image.Image,\n",
    "    img2: PIL.Image.Image\n",
    ") -> PIL.Image.Image:\n",
    "    assert img1.size[1] == img2.size[1]\n",
    "    h, w = img1.size[1], img1.size[0]\n",
    "    w2 = img2.size[0]\n",
    "    img = Image.new('RGB', (w + w2, h))\n",
    "    img.paste(img1, (0, 0))\n",
    "    img.paste(img2, (w, 0))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_poisson_blending(\n",
    "    original_img: PIL.Image.Image,\n",
    "    tryon_img: PIL.Image.Image,\n",
    "    mask_img: PIL.Image.Image\n",
    ") -> PIL.Image.Image:\n",
    "    w, h = original_img.size\n",
    "    original_img = np.array(original_img)\n",
    "    tryon_img = np.array(tryon_img)\n",
    "    mask_img = np.array(mask_img)\n",
    "    mask_img = 255 - mask_img\n",
    "    output = cv2.seamlessClone(original_img, tryon_img, mask_img, (w//2, h//2), cv2.NORMAL_CLONE)\n",
    "    return Image.fromarray(output, mode='RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Download models from Huggingface Hub\n",
    "\"\"\"\n",
    "PROJECT_ROOT_PATH = get_project_root()\n",
    "repo_id = 'bui/Navier-1'\n",
    "model_name = 'navier-1-beta-1512'\n",
    "ckpt_name = 'ckpt-132000'\n",
    "base_folder = os.path.join(model_name, ckpt_name)\n",
    "\n",
    "# unet\n",
    "unet_path = hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    subfolder=os.path.join(base_folder, 'unet'),\n",
    "    filename='diffusion_pytorch_model.safetensors',\n",
    "    local_dir=os.path.join(PROJECT_ROOT_PATH, 'checkpoints', 'navier-1')\n",
    ")\n",
    "hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    subfolder=os.path.join(base_folder, 'unet'),\n",
    "    filename='config.json',\n",
    "    local_dir=os.path.join(PROJECT_ROOT_PATH, 'checkpoints', 'navier-1')\n",
    ")\n",
    "\n",
    "# vae\n",
    "hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    subfolder=os.path.join(base_folder, 'vae'),\n",
    "    filename='diffusion_pytorch_model.safetensors',\n",
    "    local_dir=os.path.join(PROJECT_ROOT_PATH, 'checkpoints', 'navier-1')\n",
    ")\n",
    "hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    subfolder=os.path.join(base_folder, 'vae'),\n",
    "    filename='config.json',\n",
    "    local_dir=os.path.join(PROJECT_ROOT_PATH, 'checkpoints', 'navier-1')\n",
    ")\n",
    "# scheduler\n",
    "hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    subfolder=os.path.join(base_folder, 'scheduler'),\n",
    "    filename='scheduler_config.json',\n",
    "    local_dir=os.path.join(PROJECT_ROOT_PATH, 'checkpoints', 'navier-1')\n",
    ")\n",
    "# model_index.json\n",
    "hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    subfolder=base_folder,\n",
    "    filename='model_index.json',\n",
    "    local_dir=os.path.join(PROJECT_ROOT_PATH, 'checkpoints', 'navier-1')\n",
    ")\n",
    "\n",
    "model_root_path = os.path.dirname(os.path.dirname(unet_path))\n",
    "model_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\n",
    "    model_root_path,\n",
    "    subfolder='vae',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "scheduler = DDPMScheduler.from_pretrained(\n",
    "    model_root_path,\n",
    "    subfolder='scheduler'\n",
    ")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    model_root_path,\n",
    "    subfolder='unet',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "init_attn_processor(unet, cross_attn_cls=SkipAttnProcessor)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "human_parser = Parsing(gpu_id)\n",
    "openpose = OpenPose(gpu_id)\n",
    "masker = Masker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitonhd = VITONHDDataset(\n",
    "    data_rootpath=os.path.join(PROJECT_ROOT_PATH, 'datasets', 'vitonhd'),\n",
    "    use_trainset=False,\n",
    "    height=h,\n",
    "    width=w,\n",
    "    use_CLIPVision=True,\n",
    "    use_dilated_relaxed_mask=True\n",
    ")\n",
    "\n",
    "dresscode = DressCodeDataset(\n",
    "    data_rootpath='/hosjiu/data/DressCode/',\n",
    "    phase='test',\n",
    "    h=h,\n",
    "    w=w,\n",
    "    use_dilated_relaxed_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img_path = testset[14]['original_image_path']\n",
    "# img_path = '../assets/man1_good.jpg'\n",
    "# img = Image.open(img_path)\n",
    "# img = PIL.ImageOps.cover(img, (w, h))\n",
    "category = 'dresses'\n",
    "img_path = f'/hosjiu/data/DressCode/{category}/images/052001_0.jpg'\n",
    "img = Image.open(img_path)\n",
    "img = PIL.ImageOps.fit(img, size=(w, h))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask1 = False\n",
    "\n",
    "if mask1:\n",
    "    keypoints, _ = openpose(img)\n",
    "    body_parse, _  = human_parser(img)\n",
    "    mask_img, _, _, _ = get_mask_location(\n",
    "        model_type='hd',\n",
    "        category=category,\n",
    "        model_parse=body_parse,\n",
    "        keypoint=keypoints,\n",
    "        width=w,\n",
    "        height=h\n",
    "    )\n",
    "else:\n",
    "    mask_img = masker.create_mask(img, category)\n",
    "\n",
    "mask_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_arr = np.array(mask_img)\n",
    "mask_arr = np.stack([mask_arr] * 3)\n",
    "mask_arr = rearrange(mask_arr, 'c h w -> h w c')\n",
    "masked_img = np.where(mask_arr, np.array(img) * 0, np.array(img))\n",
    "masked_img = Image.fromarray(masked_img)\n",
    "masked_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = apply_net.create_argument_parser().parse_args((\n",
    "    'show',\n",
    "    '../configs/densepose_rcnn_R_50_FPN_s1x.yaml',\n",
    "    '../checkpoints/densepose/model_final_162be9.pkl',\n",
    "    img_path,\n",
    "    'dp_segm',\n",
    "    '-v'\n",
    "))\n",
    "densepose_np, instances, matrix_vis, mask_bg, _ = args.func(args, img)\n",
    "dense = Image.fromarray(densepose_np[:, :, ::-1])\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outfit_path = f'/hosjiu/data/DressCode/dresses/images/020720_1.jpg'\n",
    "# outfit_path = '../datasets/vitonhd/test/cloth/00008_00.jpg'\n",
    "outfit = Image.open(outfit_path).resize((w, h))\n",
    "outfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = pil_to_tensor(mask_img).unsqueeze(0)\n",
    "densepose = torch.from_numpy(densepose_np).permute(2, 0, 1).unsqueeze(0)\n",
    "# cloth = testset[idx]['cloth_raw'].unsqueeze(0)\n",
    "cloth = VITONHDDataset.preprocess(outfit, w, h).unsqueeze(0)\n",
    "image = VITONHDDataset.preprocess(img, w, h).unsqueeze(0)\n",
    "\n",
    "print(f'mask: {mask.shape}')\n",
    "print(f'densepose: {densepose.shape}')\n",
    "print(f'cloth: {cloth.shape}')\n",
    "print(f'img: {image.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = TryOnPipeline(\n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    scheduler=scheduler\n",
    ").to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with torch.amp.autocast(device):\n",
    "        images = pipe(\n",
    "            image=image.to(device),\n",
    "            mask_image=mask.to(device),\n",
    "            densepose_image=densepose.to(device),\n",
    "            cloth_image=cloth.to(device),\n",
    "            height=h,\n",
    "            width=w,\n",
    "            generator=torch.manual_seed(1996),\n",
    "            guidance_scale=1.3,\n",
    "        ).images\n",
    "\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poisson = apply_poisson_blending(img, images[0], mask_img)\n",
    "poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_path = os.path.join(PROJECT_ROOT_PATH, 'results', 'navier-1', 'beta', 'final', 'in_the_wilds')\n",
    "# os.makedirs(save_path, exist_ok=True)\n",
    "# img_name = img_path.split('.')[-2].split('/')[-1]\n",
    "\n",
    "# fname = f'{img_name}-{idx}.png'\n",
    "# images[0].save(Path(save_path, fname))\n",
    "\n",
    "# fname = f'{img_name}-{idx}-poisson.png'\n",
    "# poisson.save(Path(save_path, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original_image = Image.open('../assets/man10.jpg').resize((w, h))\n",
    "# tryon_img = Image.open('../results/navier-1/beta/final/in_the_wilds/man10-7-poisson.png').resize((w, h))\n",
    "# original_cloth = Image.open(testset[7]['original_cloth_path']).resize((w, h))\n",
    "\n",
    "out = functools.reduce(merge, [poisson, mask2agn(np.array(mask_img), img), img, outfit])\n",
    "# fname = 'merged-man10-7-poisson.png'\n",
    "# out.save(Path(save_path, fname))\n",
    "# out.save(f'../results/test/navier-1-beta-1512_{ckpt_name}.png')\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": ".venv",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "venv-latest",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
