{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12e8b52-e5dd-4fc3-8d8a-43b26bf155d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from fashion_clip.fashion_clip import FashionCLIP\n",
    "from os import path as osp\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPTokenizerFast, CLIPImageProcessor,\n",
    "    CLIPModel,\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPVisionModelWithProjection\n",
    ")\n",
    "from src.utils import get_project_root\n",
    "\n",
    "PROJECT_ROOT_PATH = get_project_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231f4c3-3a02-4bb7-8c5b-23faf47f5db1",
   "metadata": {},
   "source": [
    "### Export model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29aa85e-8999-4527-ac52-c46086eaa9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vision CLIP\n",
    "clip_vision_model = CLIPVisionModelWithProjection.from_pretrained('patrickjohncyh/fashion-clip')\n",
    "clip_image_processor = CLIPImageProcessor()\n",
    "inputs = clip_image_processor.preprocess(img, return_tensors='pt')['pixel_values'][0].unsqueeze(0)\n",
    "output = clip_vision_model(inputs)['image_embeds']\n",
    "\n",
    "# Export Vision CLIP model to ONNX\n",
    "torch.onnx.export(\n",
    "    model=clip_vision_model,\n",
    "    args=inputs,\n",
    "    f='../tmp/fashion_clip_image.onnx',\n",
    "    input_names=['input'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85da7368-dbba-4aed-a04d-dbcd2e6f3445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load ONNX model and test\n",
    "ort_sess = ort.InferenceSession('../tmp/fashion_clip_image.onnx', providers=['CUDAExecutionProvider'])\n",
    "img_embed = ort_sess.run(None, {'input': torch.Tensor.numpy(inputs)})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb357355-8711-4cde-a1fe-4cabd5eecd01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/H-engine/.venv39/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/home/jupyter/H-engine/.venv39/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/jupyter/H-engine/.venv39/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Text CLIP\n",
    "clip_text_model = CLIPTextModelWithProjection.from_pretrained('patrickjohncyh/fashion-clip')\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained('patrickjohncyh/fashion-clip')\n",
    "texts = ['Short Dress', 'Long Dress']\n",
    "inputs = tokenizer(text=texts, return_tensors='pt')\n",
    "output = clip_text_model(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    # attention_mask=inputs['attention_mask']\n",
    ")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=clip_text_model,\n",
    "    args=(inputs['input_ids']),\n",
    "    f='../tmp/fashion_clip_text.onnx',\n",
    "    input_names=['input']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59ee825b-e162-4896-8038-04d39feb2075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0753,  0.1137, -0.2012,  ...,  0.2871, -0.1401, -0.1745],\n",
       "        [ 0.1993,  0.0851, -0.2964,  ..., -0.0155, -0.2383, -0.0200]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "565c0e90-1335-44fb-af18-6b99e150e730",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.07526917,  0.1137128 , -0.20121977, ...,  0.2870652 ,\n",
       "         -0.14005005, -0.17451927],\n",
       "        [ 0.19929732,  0.08510898, -0.29637247, ..., -0.0154532 ,\n",
       "         -0.23826578, -0.02000891]], dtype=float32),\n",
       " array([[[ 0.08299014,  0.06903712,  0.3631887 , ..., -0.0869887 ,\n",
       "           0.22818953,  0.47101068],\n",
       "         [ 1.97926   ,  0.32932737,  1.1483428 , ..., -1.5879972 ,\n",
       "           0.7777781 , -0.25711507],\n",
       "         [ 2.6857781 ,  0.8550246 ,  1.7345062 , ..., -0.7738764 ,\n",
       "          -0.6795901 , -0.28669494],\n",
       "         [ 1.0880939 ,  1.0650144 ,  0.52067816, ..., -2.678224  ,\n",
       "          -0.13050346, -1.3930943 ]],\n",
       " \n",
       "        [[ 0.08299014,  0.06903712,  0.3631887 , ..., -0.0869887 ,\n",
       "           0.22818953,  0.47101068],\n",
       "         [-0.64101744, -0.4311428 , -0.48019487, ...,  0.6071063 ,\n",
       "           0.8231285 , -0.4630533 ],\n",
       "         [ 2.259399  ,  1.0240291 ,  1.4451071 , ..., -0.05386524,\n",
       "          -0.10072777, -0.77439076],\n",
       "         [ 1.8626235 ,  0.7605586 ,  0.63829696, ..., -1.2113088 ,\n",
       "           0.09883521, -0.1344884 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_numpy = lambda x: torch.Tensor.numpy(x)\n",
    "ort_sess = ort.InferenceSession('../tmp/fashion_clip_text.onnx', provider=['CUDAExecutionProvider'])\n",
    "ort_sess.run(None, {'input': to_numpy(inputs['input_ids'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5603db2-db1e-4286-a718-17240fc34e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])\n",
      "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# Create CLIP-based model and try inference\n",
    "# model = CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\n",
    "# processor = CLIPProcessor.from_pretrained('patrickjohncyh/fashion-clip')\n",
    "# img = Image.open('../assets/damngan2.png')\n",
    "# img = ImageOps.fit(img, size=((384, 512)))\n",
    "# text = ['Short Dress', 'Long Dress']\n",
    "# inputs = processor(text=text, images=img, return_tensors='pt',)\n",
    "# outputs = model(**inputs)\n",
    "# print(inputs.keys())\n",
    "# print(outputs.keys())\n",
    "# print(outputs['image_embeds'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bae715a-a4fa-443f-a260-3da3f25be8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.onnx.export(\n",
    "#     model=model,\n",
    "#     args=(inputs['input_ids'], inputs['pixel_values']),\n",
    "#     f='../tmp/fashion_clip.onnx',\n",
    "#     input_names=['input'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e302fee-6235-4767-9837-692087643854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ort_sess = ort.InferenceSession('../tmp/fashion_clip.onnx', providers=['CUDAExecutionProvider'])\n",
    "# inputs = processor(text=text, images=img, return_tensors='pt',)\n",
    "# ort_sess.run(None, {'input': (to_numpy(inputs['input_ids']), to_numpy(inputs['pixel_values']))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1e555-897a-460f-8145-03be5302d3c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = osp.join(PROJECT_ROOT_PATH, 'checkpoints/fashion_clip/model.onnx')\n",
    "\n",
    "ort_sess = ort.InferenceSession(MODEL_PATH)\n",
    "processor = CLIPProcessor.from_pretrained('patrickjohncyh/fashion-clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d881df-8439-4cf3-bc77-133523720eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = Image.open('../assets/damngan2.png')\n",
    "img = ImageOps.fit(img, size=((384, 512)))\n",
    "\n",
    "# Inputs\n",
    "text = ['short dress', 'long dress']\n",
    "inputs = processor(text=text, images=img, return_tensors='pt', padding=True)\n",
    "\n",
    "ort_sess.run(None, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba2ad7-f51c-453d-b0f4-273c69453398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fclip = FashionCLIP('fashion-clip')\n",
    "\n",
    "img = Image.open('../assets/damngan2.png')\n",
    "ImageOps.fit(img, size=((192, 384)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02622fe-08db-44ed-9917-103c5321920e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_embed = fclip.encode_images([img], batch_size=1)\n",
    "normalized_img_embed = img_embed / np.sqrt(np.sum(img_embed**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67dfe0-dd97-49fa-8aa0-55baaf5b19c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "types = ['short dress', 'long dress']\n",
    "types_embed = fclip.encode_text(types, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a56e7-76d3-469e-8140-4c2c90fe6763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm = np.sqrt(np.sum(types_embed**2, axis=1))\n",
    "normalized_types_embed = types_embed / np.expand_dims(norm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8313e6f-4bbc-4231-b1fa-88082918de74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = np.dot(normalized_types_embed, np.transpose(img_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a81cc4-90d8-4f78-851c-6e3d7e89f435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = np.argmax(output)\n",
    "if idx == 0:\n",
    "    print(types[0])\n",
    "elif idx == 1:\n",
    "    print(types[1])\n",
    "else:\n",
    "    raise ValueError(f'Value {idx} is not supported.')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": ".venv39",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "venv39 (Local)",
   "language": "python",
   "name": ".venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
